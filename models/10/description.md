# Iteration 10
32m data point
cost: score of round + score of hand
_different card encoding ;training vs simple and small offline_

## Results
### Round 1: Trained on 32m offline and 288 online samples (mlp vs simple)
mlp vs sim: 26% trained NOT playing the best card
mlp vs sim: 17% trained playing the best card

*Conclusions*:
- after round 1, the results are slightly better than at iteration 8
- again, training by playing the best card works less well


