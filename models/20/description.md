# Relative encoding
Using encoding 13 from binary data with new baseline "better"
2M stored hands initially (simple vs better) -> afterwards, it should be from baseline vs baseline
Round: 2M offline from other model in previous round + 2M online
*Playing best card during training*
Storing samples that are played
Only using normal parameters
Training a 5x100-model on the data generated by the other 2 models as buffer which doesn't play own hands

# Round 1
=> 4M hands
3x100: 45.0/54.2%, loss: 1651.2
4x100: 48.1/56.6%, loss: 1653.9

# Round 1.5
=> 6M hands
5x100: 57.5%, loss: 1611.9

# Round 2
=> 8M hands
3x100: 53.6/54.4%, loss: 1580.4
4x100: 59.8/59.9%, loss: 1627.0

# Round 2.5
=> 10M hands
5x100: 59.8%, loss: 1586.3

# Round 3
=> 12M hands
3x100: 58.4/56.5%, loss: 1573.0
4x100: 60.8/62.0%, loss: 1591.2

# Round 3.5
=> 14M hands
5x100: 58.0%, loss: 1562.1

# Round 4
=> 16M hands
3x100: 60.8/58.1%, loss: 1569.0
4x100: 64.2/62.6%, loss: 1591.1

# Round 4.5
=> 18M hands
5x100: 56.7%, loss: 1573.8

# Round 5
=> 20M hands
3x100: 60.4/59.3%, loss: 1565.6
4x100: 64.3/62.9%, loss: 1576.9

# Round 5.5
=> 22M hands
5x100: 60.2%, loss: 1563.5

# Round 6
=> 24M hands
3x100: 58.6/59.6%, loss: 1578.6
4x100: 63.9/64.0%, loss: 1569.9

# Round 6.5
=> 26M hands
5x100: 59.6%, loss: 1559.0

# Round 7
=> 28M hands
3x100: 61.0/59.6%, loss: 1550.5
4x100: 65.0/64.9%, loss: 1522.9

# Round 7.5
=> 30M hands
5x100: 59.3%, loss: 1531.3

# Round 8
=> 32M hands
3x100: 62.4/59.8%, loss: 1559.3
4x100: 63.8/65.4%, loss: 1550.7

# Round 8.5
=> 34M hands
5x100: 58.2%, loss: 1562.7

# Round 9
=> 36M hands
3x100: 63.4/60.2%, loss: 1560.4
4x100: 66.1/65.5%, loss: 1554.7

# Round 9.5
=> 38M hands
5x100: 61.4%, loss: 1565.2

# Round 10
=> 40M hands
3x100: 62.2/60.0%, loss: 1557.1
4x100: 66.2/65.8%, loss: 1523.0

# Round 10.5
=> 42M hands
5x100: 61.6%, loss: 1534.2

