# Trying to see how the different data compare when training
Training new 3x100-models
Using "default" encoding (5) from binary data
1M stored hands, 2M online -> 2M is rather little, but in order to get the results in time...
Training data is generated by the 3x100 MLP from 13

## Online training as usual (not playing best)
sim-vs-sim: 59.8%, loss after offline: 556.9, after first online: 775.8, after online: 671.1
mlp-vs-sim: 58.4%, loss after offline: 712.7, after first online: 723.8, after online: 680.3
mlp-best-vs-sim: 57.5%, loss after offline: 537.9, after first online: 760.4, after online: 668.2

## Online training playing best
sim-vs-sim: 59.3%, loss after offline: 556.9, after first online: 655.0, after last: 528.3
mlp-vs-sim: 57.6%, loss after offline: 712.7, after first online: 572.5, after last: 534.1
mlp-best-vs-sim: 57.8%, loss after offline: 537.9, after first online: 648.7, after last: 524.5

# Same, but with more data
Qualitative findings:
- training on sim-data seems to beat no initial training (after no offline and 3M of online hands,
  it seems to be about at the level it would be after 3M offline-vs-sim hands)
- training on sim-data seems to be equivalent to training on mlp data, except that loss on mlp-data
  during offline training is close to the loss of online-training
- playing the best card during training seems to lower loss but not significantly improve performance

# Overfitting
Using "default" encoding (5) from binary data
9M stored hands, 6M online
Training data is generated by the 3x100 MLP from 13
Default alpha: 1e-4
Default Learning rate init: 1e-3
Results of the first set of evals were inconclusive, so I'm staying with the default
training-without-playing-best setting.

3x100, alpha 1e-2: 63.7%, loss: 643.5 (training time: 23h15)
-> untrained: 51.0%, loss: 499.2 -> 685.4
3x100, alpha 1e-3: 62.1%, loss: 648.3 (training time: 22h45)
-> untrained: 54.4%, loss: 499.0 -> 685.5
3x100, alpha 1e-4: 66.1%, loss: 645.0 (training time: 22h45)
-> untrained: 49.1%, loss: 497.3 -> 688.7

4x100, alpha 1e-2: 66.4%, loss: 637.0 (training time: 34h30)
-> untrained: 52.1%, loss: 494.0 -> 679.2 ==> looks like it might still learn
4x100, alpha 1e-3: 64.8%, loss: 638.0 (training time: 33h45)
-> untrained: 58.0%, loss: 494.9 -> 680.6
4x100, alpha 1e-4: 64.3%, loss: 632.8 (training time: 34h)
-> untrained: 57.3%, loss: 491.4 -> 675.2

==> alpha 1e-4 might be slightly better

# Overfitting, take 2
Using "default" encoding (5) from binary data
3M stored hands (mlp-vs-sim), 3M online
Training data is generated by the 3x100 MLP from 13
Default alpha: 1e-4
Default Learning rate init: 1e-3
Results of the first set of evals were inconclusive, so I'm staying with the default
training-without-playing-best setting.
Alternative model architectures

100-10-100-10-100: 46.5%, loss: 714.3 (training time: 10h15) => probably underfitting
100-20-100-20-100: 54.9%, loss: 690.9 (training time: 10h15) => probably underfitting
100-50-100-50-100: 64.8%, loss: 659.5 (training time: 12h) ==> looks like it may still be learning

# Overfitting, take 3
Using "default" encoding (5) from binary data
3M stored hands (mlp-vs-sim), 3M online
Training data is generated by the 3x100 MLP from 13
Default alpha: 1e-4
Default Learning rate init: 1e-3
Default max iter: 200
Results of the first set of evals were inconclusive, so I'm staying with the default
training-without-playing-best setting.
Varying max\_iter, learning\_rate\_init, and batch\_size

3x100, max\_iter 100: 57.7%, loss: 672.1 (training time: 11h30) ==> might be learning more slowly and with less overfitting (or just not learn enough)
-> after another 3M online training: 59.2% => doesn't seem that promising...
3x100, max\_iter 300: 61.2%, loss: 668.5 (training time: 11h45) => similar look of lc to max\_iter=100 -> this param may have little impact

3x100, lri 1e-2: 36.6%, loss: 742.2 (training time: 13h30) => might be WAY overfitting
3x100, lri 1e-4: 46.1%, loss: 727.6 (training time: 8h45) => probably doesn't learn enough

3x100, batch\_size 100: 58.1%, loss:  (training time: 14h30) => doesn't seem to do much
3x100, batch\_size 400: 61.0%, loss: 651.0 (training time: 10h15) => doesn't seem to do much

3x100, stopping: can't run (exception)

