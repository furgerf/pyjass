# Iteration 9
32m data point
cost: score of round + score of hand
_same as 7 but training vs simple and small offline_
*IGNORE, as this is the same as iteration 8*

## Results
### Round 1: Trained on 32m offline and 288 online samples (mlp vs simple)
mlp vs sim: 14% trained playing the best card
mlp vs sim: 22% trained NOT playing the best card

*Conclusions*:
-

### Round 2: Based on round 1 plus 320m vs simple
mlp vs sim: 17% trained playing the best card both times
mlp vs sim: x% trained NOT playing the best card both times <-- aborted

*Conclusions*:
-

